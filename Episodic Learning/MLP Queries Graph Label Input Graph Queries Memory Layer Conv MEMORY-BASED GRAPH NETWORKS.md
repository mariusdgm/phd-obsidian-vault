[) MLP Queries Graph Label Input Graph Queries Memory Layer Conv | Semantic Scholar](https://www.semanticscholar.org/paper/)-MLP-Queries-Graph-Label-Input-Graph-Queries-Layer/5f101cdb4150429c1a7be1ed8704c9dc80fac4ca#cited-papers)

Graph Neural Networks (GNNs) are a class of deep models that operates on data with arbitrary topology and order-invariant structure represented as graphs. We introduce an efficient memory layer for GNNs that can learn to jointly perform graph representation learning and graph pooling. We also introduce two new networks based on our memory layer: Memory-Based Graph Neural Network (MemGNN) and Graph Memory Network (GMN) that can learn hierarchical graph representations by coarsening the graph throughout the layers of memory. The experimental results demonstrate that the proposed models achieve state-of-the-art results in six out of seven graph classification and regression benchmarks. We also show that the learned representations could correspond to chemical features in the molecule data.

Unlike RNNs in which the memory is represented within the hidden states of the network, the explicit representation in the memory-augmented models helps them to store and retrieve longer-term memories with less parameters. The memory is usually implemented either as a differentiable neural dictionary (i.e., key-value memory) (e.g., neural episodic control (Pritzel et al., 2017) and product-key memory layers (Lample et al., 2019)) or a differentiable array (e.g., Neural Turing Machine (NTM) (Graves et al., 2014), prototypical networks (Snell et al., 2017), and memory networks (Weston et al., 2015)). Our memory layer consists of a multi-head array and a convolutional layer to aggregate the heads.

Graph datasets, not Atari