[[PDF] Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks | Semantic Scholar](https://www.semanticscholar.org/paper/Sparsity-in-Deep-Learning%3A-Pruning-and-growth-for-Hoefler-Alistarh/9d6acac70b2d1fdb861a08b00766ef263109cd7f)

The contributions of this paper are:

1. A survey of prior work on sparsity in deep learning.
2. An extensive tutorial of sparsification for both inference and training.
3. Description of approaches to remove and add elements of neural networks, different training strategies to achieve model sparsity, and mechanisms to exploit sparsity in practice.
4. Inclusion of necessary background on mathematical methods in sparsification, description of phenomena such as early structure adaptation, the intricate relations between sparsity and the training process, and techniques for achieving acceleration on real hardware.
5. Definition of a metric of pruned parameter efficiency that could serve as a baseline for comparison of different sparse networks.
6. Speculation on how sparsity can improve future workloads and outlining major open problems in the field.

This paper provides a literature survey of prior work on sparsity in deep learning. It summarizes more than 300 research papers and distills ideas for practitioners who wish to utilize sparsity today, as well as for researchers whose goal is to push the frontier forward. The survey covers approaches to remove and add elements of neural networks, different training strategies to achieve model sparsity, and mechanisms to exploit sparsity in practice. It also includes the necessary background on mathematical methods in sparsification, describes phenomena such as early structure adaptation, the intricate relations between sparsity and the training process, and shows techniques for achieving acceleration on real hardware.