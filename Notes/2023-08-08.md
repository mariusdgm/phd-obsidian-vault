[[The Dormant Neuron Phenomenon in Deep Reinforcement Learning]] 

1. **Dying ReLU Problem**: Since the output is zero for all negative inputs, it's possible during training that some neurons can sometimes always output a negative value, leading them to be inactive and not adjust their weights. Essentially, these neurons are not learning from the data. This issue is called the "dying ReLU" problem.

To address the dying ReLU problem, several variants of the ReLU function have been proposed:

- **Leaky ReLU**: Instead of defining the function as 0 for negative values, it's defined as a small negative slope (like 0.01). So it avoids having completely inactive units in the network.
    
- **Parametric ReLU (PReLU)**: Similar to Leaky ReLU, but �α is learned during training. This means that the model can adaptively learn the value of �α for each neuron.
    
- **Exponential Linear Units (ELU)**: Tries to make the mean activations closer to zero, which speeds up training.


Florin did experiments, inconclusive but ReLu somehow seems to be better then the other variants

![[Pasted image 20230808155959.png]]