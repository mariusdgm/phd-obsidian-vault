[[PDF] WeightMom: Learning Sparse Networks using Iterative Momentum-based pruning | Semantic Scholar](https://www.semanticscholar.org/paper/WeightMom%3A-Learning-Sparse-Networks-using-Iterative-Johnson-Tang/a630c13a64de3fb37a6ac92db125acd2d2961267)

Deep Neural Networks have been used in a wide variety of applications with signiﬁcant success. However, their highly complex nature owing to comprising millions of parame- ters has lead to problems during deployment in pipelines with low latency requirements. As a result, it is more desirable to obtain lightweight neural networks which have the same performance during inference time. In this work, we propose a weight based pruning approach in which the weights are pruned gradually based on their momentum of the previous iterations. Each layer of the neural network is assigned an importance value based on their relative sparsity, followed by the magnitude of the weight in the previous iterations. We evaluate our approach on networks such as AlexNet, VGG16 and ResNet50 with image classiﬁcation datasets such as CIFAR-10 and CIFAR-100. We found that the results outperformed the previous approaches with respect to accuracy and compression ratio. Our method is able to obtain a compression of 15 × for the same degradation in accuracy on both the datasets.

Conclusions: we proposed WeightMom an iterative weight based pruning strategy based on the momentum of the weight magnitudes across the previous few iterations. we found that while maintaining the same test accuracyL our proposed approach was able to better preserve the important parameters in the network especially at extremely high compression rates of over 10% we believe that using momentum to prune weights can be an extremely useful tool to determine which parameters are important to the network since it takes the performance of the weight over a period of time before determining the importance of the parameter rather than discarding the parameter based on a single examination.