We demonstrate the possibility of what we call sparse learning: accelerated training of deep neural networks that maintain sparse weights throughout training while achieving dense performance levels. We accomplish this by developing sparse momentum, an algorithm which uses exponentially smoothed gradients (momentum) to identify layers and weights which reduce the error efficiently. Sparse momentum redistributes pruned weights across layers according to the mean momentum magnitude of each layer. Within a layer, sparse momentum grows weights according to the momentum magnitude of zero-valued weights. We demonstrate state-of-the-art sparse performance on MNIST, CIFAR-10, and ImageNet, decreasing the mean error by a relative 8%, 15%, and 6% compared to other sparse algorithms. Furthermore, we show that sparse momentum reliably reproduces dense performance levels while providing up to 5.61x faster training. In our analysis, ablations show that the benefits of momentum redistribution and growth increase with the depth and size of the network. Additionally, we find that sparse momentum is insensitive to the choice of its hyperparameters suggesting that sparse momentum is robust and easy to use.

The literature survey of this paper discusses the history of creating sparse neural networks from dense ones. Earlier work focused on pruning via second-order derivatives and heuristics to ensure efficient training of networks after pruning. Recent work is motivated by the memory and computational benefits of sparse models that enable the deployment of deep neural networks on mobile and low-energy devices. The iterative train-dense, prune, re-train cycle introduced by Han et al. has been a very influential paradigm. Extensions to this work include compressing recurrent neural networks and other models, continuous pruning and re-training, joint loss/pruning-cost optimization, layer-by-layer pruning, fast-switching growth-pruning cycles, and soft weight-sharing.

The contributions of this paper are:

- Introducing the concept of sparse learning, which is the training of deep neural networks that maintain sparse weights throughout training while achieving dense performance levels.
- Developing a new algorithm called sparse momentum, which uses exponentially smoothed gradients (momentum) to identify layers and weights that reduce the error efficiently and redistributes pruned weights across layers according to the mean momentum magnitude of each layer.
- Demonstrating state-of-the-art sparse performance on MNIST, CIFAR-10, and ImageNet-1k datasets, achieving a relative decrease in mean error of 8%, 15%, and 6% compared to other sparse algorithms.
- Showing that sparse momentum reliably reproduces dense performance levels while providing up to 5.61x faster training.
- Determining the percentage of weights needed to reach dense performance levels for different network architectures on CIFAR-10 dataset, which is between 35-50%, 5-10%, and 20-30% weights for AlexNet, VGG16, and Wide Residual Networks, respectively.

There are a few limitations of this paper:

- The experiments are limited to a few datasets and network architectures, so the generalizability of the results to other datasets and architectures is not clear.
- The paper does not provide a detailed analysis of the computational and memory requirements of sparse momentum compared to other sparse algorithms.
- The paper does not explore the effect of different hyperparameters on the performance of sparse momentum in depth. However, the authors show that sparse momentum is relatively insensitive to the choice of hyperparameters.
- The paper does not compare the performance of sparse momentum to other state-of-the-art sparse algorithms that use pruning and retraining.